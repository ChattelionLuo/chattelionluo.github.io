---
title: 'Blog Post number 3'
date: 2014-08-14
permalink: /posts/2014/08/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======

In this tutorial, we are going to implement a $L$ layer neural network (multiple layer perceptron) that can perform simple tasks such as digit recognization. $L$ is an arbitrary number as you require, within which we want $L-1$ weight matrices, and the last one is the output layer (e.g. a Softmax). We define a data matrix $\boldsymbol{X}$ ($N$ by $M$) that contains $N$ piece of data with dimension $M$; We $\textcolor{red}{strictly}$ follow \cite{xu} step by step via specifying every component and giving MATLAB implementation. Some definitions of variables.  
$N$, number of training data;  
$K$, number of categories;  
$M$, dimension of data;  
$\boldsymbol{X}$, data matrix ($N$ by $M$);  
$\boldsymbol{Y}$, a matrix of One Hot labels ($N$ by $K$);  
$L$, number of layers (including input and output layer);  
$\boldsymbol{w}^{(i)}$, weight matrix (with bias) linking the $(i)$-th and $(i+1)$-th layer;  
$\textcolor{red}{\text{Pay attention}}$: we only have $\textcolor{\text{red}}{(L-1)}$ number of weight matrices.  
$\boldsymbol{z}^{(i)}=\boldsymbol{a}^{(i-1)}\boldsymbol{w}^{(i-1)}$, pre-activation $\textcolor{red}{\text{at}}$ the $i$-th layer;  
$\boldsymbol{a}^{(i)}=\sigma(\boldsymbol{z}^{(i)})$, activated product $\textcolor{red}{\text{at}}$ the $i$-th layer;  
$\alpha$, constant learning rate;  

You can have many headings
======

Aren't headings cool?
------
