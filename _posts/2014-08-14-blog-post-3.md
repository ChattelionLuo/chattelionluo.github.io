---
title: 'Blog Post number 3'
date: 2014-08-14
permalink: /posts/2014/08/blog-post-3/
tags:
  - cool posts
  - category1
  - category2
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======

In this tutorial, we are going to implement a $L$ layer neural network (multiple layer perceptron) that can perform simple tasks such as digit recognization. $L$ is an arbitrary number as you require, within which we want $L-1$ weight matrices $\boldsymbol{w}^{(i)}$ that link the $i$-th and the $i+1$-th layer, and the last one is the output layer (e.g. a Softmax). We define a data matrix $\boldsymbol{X}$ ($N$ by $M$) that contains $N$ pieces of data with dimension $M$, and a target $\boldsymbol{Y}$ ($N$ by $K$), a matrix of $K$-category one-hot label. We $\textcolor{red}{strictly}$ follow \cite{xu} step by step via specifying every component and giving MATLAB implementation. Some definitions of variables.
$\boldsymbol{z}^{(i)}=\boldsymbol{a}^{(i-1)}\boldsymbol{w}^{(i-1)}$, pre-activation $\textcolor{red}{\text{at}}$ the $i$-th layer;  
$\boldsymbol{a}^{(i)}=\sigma(\boldsymbol{z}^{(i)})$, activated product $\textcolor{red}{\text{at}}$ the $i$-th layer;  
$\alpha$, constant learning rate;  

You can have many headings
======

Aren't headings cool?
------

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script>
<script type="text/javascript">
  MathJax.Hub.Config ({
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$', '$'], ["\\(", "\\)"] ],
      displayMath: [ ['$$', '$$'], ["\\[","\\]"] ],
      processEscapes: true
    },
    "HTML-CSS": { availableFonts: ["LaTeX"] }
  });
</script>
