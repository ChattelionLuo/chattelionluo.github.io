---
title: 'Latex example'
date: 2023-08-17
permalink: /posts/2023/08/blog-post/
tags:
  - cool posts
  - category1
  - category2
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======

In this tutorial, we are going to implement a $L$ layer neural network (multiple layer perceptron) that can perform simple tasks such as digit recognization. $L$ is an arbitrary number as you require, within which we want $L-1$ weight matrices $\boldsymbol{w}^{(i)}$ that link the $i$-th and the $i+1$-th layer, and the last one is the output layer (e.g. a Softmax). We define a data matrix $\boldsymbol{X}$ ($N$ by $M$) that contains $N$ pieces of data with dimension $M$, and a target $\boldsymbol{Y}$ ($N$ by $K$), a matrix of $K$-category one-hot label. We $\textcolor{red}{strictly}$ follow \cite{xu} step by step via specifying every component and giving MATLAB implementation. Some definitions of variables.
$\boldsymbol{z}^{(i)}=\boldsymbol{a}^{(i-1)}\boldsymbol{w}^{(i-1)}$, pre-activation at the $i$-th layer;  
$\boldsymbol{a}^{(i)}=\sigma(\boldsymbol{z}^{(i)})$, activated product at the $i$-th layer;  
$\alpha$, constant learning rate;

These render differently. For example, type the following to show inline mode: $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$
$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$
$$\left.x^2\right\rvert_3^5 = 5^2-3^2$$  

$$ \begin{align}
  \tag{1.1}
  V_{sphere} = \frac{4}{3}\pi r^3
\end{align}  $$

$$ \begin{pmatrix}     1 & x & x^2\\ \\   1 & y & y^2\\     1 & z & z^2\\     \end{pmatrix} $$


```  
function [C,a,z] = forward(X,Y,layers,weights)
a1 = [ones(size(X,1), 1) X];                %design input
C = 0;                                      %initialise cost

for i = 1:(size(layers,2))                  %initialize the sum and the activations
    z{i} = zeros(size(X,1),layers(i)+1);
    a{i} = zeros(size(X,1),layers(i)+1);
end

a{1} = a1;                                  %input layer
for i = 2:size(layers,2)                    %hidden layers
z{i} = a{i-1}*weights{i-1}';
a{i} = z{i};
if i ~= size(layers,2)                       %(eq 1.6)
    a{i} = sigmoid(z{i});
    a{i} = [ones(length(z{i}),1) a{i}];     %append bias for next layer
    else
    a{i} = softmax(a{i});                   %output layer (eq 1.7)
    end
end

C = crossentropy(Y,a{end});                 %compute cost eq.(eq 1.8)
end  
```

$$\begin{pmatrix}a & b\\ c & d\end{pmatrix}$$

$$\begin{bmatrix}X\\Y\end{bmatrix}$$

\begin{align}
\frac{\partial}{\partial\boldsymbol{w}} \mathcal{C}(\boldsymbol{w}) &=\frac{\partial}{\partial\boldsymbol{w}} 
\end{align}

You can have many headings
======

$$\begin{align}
\tag{1.8}
  X_0 \otimes X_1 =  
  \begin{pmatrix}
    0\begin{pmatrix} 
      0 & 1 \\
      1 & 0
    \end{pmatrix} & 1\begin{pmatrix}
                      0 & 1 \\
                      1 & 0
                     \end{pmatrix} \\
    1\begin{pmatrix} 
      0 & 1 \\
      1 & 0
     \end{pmatrix} & 0\begin{pmatrix}
                       0 & 1 \\
                       1 & 0
                      \end{pmatrix} 
  \end{pmatrix} = 
  \begin{pmatrix}
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 
  \end{pmatrix}
\end{align}$$

Aren't headings cool?
------
