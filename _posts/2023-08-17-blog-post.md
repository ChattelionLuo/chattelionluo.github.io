---
title: 'Tutorial: Build your first neural network from scratch'
date: 2023-08-17
permalink: /posts/2023/08/blog-post/
tags:
  - cool posts
  - category1
  - category2
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======
In this episode, we want to investigate a simple working neural network (NN) anatomically, without the use of any deep learning package. It's very interesting to design every single element that is essential for performing forward and backward propagation. Every piece of component can be put up together like playing a LEGO game!  
we are going to implement a $L$ layer neural network (multiple layer perceptron) that can perform simple tasks such as digit recognization. $L$ is an arbitrary number as you require, within which we want $L-1$ weight matrices $\boldsymbol{w}^{(i)}$ that link the $i$-th and the $i+1$-th layer, and the last one is the output layer (e.g. a Softmax). We define a data matrix $\boldsymbol{X}$ ($N$ by $M$) that contains $N$ pieces of data with dimension $M$, and a target $\boldsymbol{Y}$ ($N$ by $K$), a matrix of $K$-category one-hot label. We follow \cite{xu} step by step via specifying every component and giving MATLAB implementation. Some definitions of variables.
$\boldsymbol{z}^{(i)}=\boldsymbol{a}^{(i-1)}\boldsymbol{w}^{(i-1)}$, pre-activation at the $i$-th layer;  
$\boldsymbol{a}^{(i)}=\sigma(\boldsymbol{z}^{(i)})$, activated product at the $i$-th layer;  
$\alpha$, constant learning rate;  
Since we are dealing with data matrices rather than vectors, to make more efficient computation we try to enable every MATLAB function to do element-wise computation with matrices.
$$\begin{equation}
\sigma(t)=\frac{1}{1+\exp (-t)}
\end{equation}$$
```
function g = sigmoid(T)
    g = 1 ./ (1+exp(-T));
end
```
Derivative of Sigmoid
\begin{align}\frac{d \sigma(t)}{d t}=\sigma(t)(1-\sigma(t))\end{align}
Softmax
$$\begin{equation}
\textbf{Softmax}\left(\boldsymbol{t}\right)=\left(\frac{\exp \left(t_1\right)}{\sum_{k=1}^K \exp \left(t_k\right)}, 
\cdots, 
\frac{\exp \left(t_K\right)}{\sum_{k=1}^K \exp \left(t_k\right)}\right)
\end{equation}$$
```
function y = softmax(T)
    y = exp(T) ./ sum(exp(T),2);        %denominator sum by row
end
```
Cross Entropy (This will be our loss function)
$$\begin{equation}
\mathcal{C}(\boldsymbol{y},\boldsymbol{\hat{y}})=\frac{1}{N} \sum_i^N \sum_j^K -y_{i k} \log \left(\hat{y}_{i k}\right)
\end{equation}$$
```
function C = crossentropy(Y,Y_hat)
C = sum(sum(-Y.* log(Y_hat))) / size(Y, 1) ;
```

Initialise Weights
======
Only need to specify size of layers (number of neurons in each layer), say, `layers = [input layer size,hidden layer ,..., hidden layer , output size]`. For every weights in hidden layers, one more column is appended as the bias coefficients. All initialization of weights entries are sampled from $\mathcal{N}(0,0.1)$.
```
function ini_weights = randWeights(layers)
for i = 1:(size(layers, 2)-1)
    epsilon = 0.1;
    ini_weights{i} = rand(layers(i+1), 1+layers(i))*2*epsilon - epsilon;
    end
end
```

Forward Propagation
===
Specify the input layer $\boldsymbol{a}^{(1)}$. By the way we introduce bias that is already contained in weights.
$$\begin{equation}
    \boldsymbol{a}^{(1)}=\left[\begin{array}{cc}
1&\boldsymbol{X}_1\\
\cdots&\cdots\\
1&\boldsymbol{X}_N
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{1}&\boldsymbol{X}\\
\end{array}\right]
\end{equation}$$
Generate sequence of hidden layers $\left\{\boldsymbol{z}^{(l)}\right\}_2^{L-1}$ , and $\left\{\boldsymbol{a}^{(l)}\right\}_2^{L-1}$ , here $\boldsymbol{1}$ represent a column vector of ones.
$$\begin{equation}
    \begin{aligned}
    \boldsymbol{z}^{(l)}&=\boldsymbol{a}^{(l-1)}\cdot\boldsymbol{w}^{(l-1)^\top}\\
    \boldsymbol{a}^{(l)}&=\left[\begin{array}{cc}
\boldsymbol{1}&\sigma(\boldsymbol{z}^{(l)})\\
\end{array}\right]
    \end{aligned}
\end{equation}$$
Compute the output layer $\boldsymbol{z}^L$ and
$\boldsymbol{a}^L$,
$$\begin{equation}
    \begin{aligned}
    \boldsymbol{z}^{(L)}&=\boldsymbol{a}^{(L-1)}\cdot\boldsymbol{w}^{(L-1)^\top}\\
    \boldsymbol{a}^{(L)}&=\textbf{Softmax}(\boldsymbol{z}^{(L)})
    \end{aligned}
\end{equation}$$
Compute the total lost,
$$\begin{equation}
    \mathcal{L}\left(\boldsymbol{Y},\boldsymbol{a}^{(L)}\right).
\end{equation}$$
These render differently. For example, type the following to show inline mode: $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$
$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$
$$\left.x^2\right\rvert_3^5 = 5^2-3^2$$  

$$\begin{align} V_{sphere} = \frac{4}{3}\pi r^3\end{align}$$
This is a sample blog post.
$$ \begin{pmatrix} 1 & x & x^2\\ 1 & y & y^2\\ 1 & z & z^2\\ \end{pmatrix} $$

This is a sample blog post.
```  
function [C,a,z] = forward(X,Y,layers,weights)
a1 = [ones(size(X,1), 1) X];                %design input
C = 0;                                      %initialise cost

for i = 1:(size(layers,2))                  %initialize the sum and the activations
    z{i} = zeros(size(X,1),layers(i)+1);
    a{i} = zeros(size(X,1),layers(i)+1);
end

a{1} = a1;                                  %input layer
for i = 2:size(layers,2)                    %hidden layers
z{i} = a{i-1}*weights{i-1}';
a{i} = z{i};
if i ~= size(layers,2)                       %(eq 1.6)
    a{i} = sigmoid(z{i});
    a{i} = [ones(length(z{i}),1) a{i}];     %append bias for next layer
    else
    a{i} = softmax(a{i});                   %output layer (eq 1.7)
    end
end

C = crossentropy(Y,a{end});                 %compute cost eq.(eq 1.8)
end  
```
This is a sample blog post.

$$\begin{pmatrix}a & b\\ c & d\end{pmatrix}$$
This is a sample blog post.
$$\begin{align}\begin{bmatrix}X\\Y\end{bmatrix}\end{align}$$
This is a sample blog post.
\begin{align}
\frac{\partial}{\partial\boldsymbol{w}} \mathcal{C}(\boldsymbol{w}) &=\frac{\partial}{\partial\boldsymbol{w}} 
\end{align}

You can have many headings
======
This is a sample blog post.
$$\begin{align}
  X_0 \otimes X_1 =  
  \begin{pmatrix}
    0\begin{pmatrix} 
      0 & 1 \\
      1 & 0
    \end{pmatrix} & 1\begin{pmatrix}
                      0 & 1 \\
                      1 & 0
                     \end{pmatrix} \\
    1\begin{pmatrix} 
      0 & 1 \\
      1 & 0
     \end{pmatrix} & 0\begin{pmatrix}
                       0 & 1 \\
                       1 & 0
                      \end{pmatrix} 
  \end{pmatrix} = 
  \begin{pmatrix}
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 
  \end{pmatrix}
\end{align}$$


This is a sample blog post.
$$\begin{align}
\frac{\partial}{\partial\boldsymbol{w}} \mathcal{C}(\boldsymbol{w}) &=\frac{\partial}{\partial\boldsymbol{w}} \left(\frac{1}{N}\sum_{i=1}^N \sum_{k=1}^K -y_{i, k}\left[\log \left(\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_k\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)}\right)\right]\right) \\
&=\frac{1}{N}\left[\begin{array}{ccc}
\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_1\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)} &
\cdots &
\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_K\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)}
\end{array}\right]-\frac{1}{N}\boldsymbol{y}_i \\
& \equiv \frac{1}{N} \left[\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)}-\boldsymbol{y}_i\right]\\
&  \equiv \frac{1}{N}\left(\boldsymbol{a}^{(L)}-\boldsymbol{Y}\right).
\end{align}$$
This is a sample blog post.


Aren't headings cool?
------
