---
title: 'Tutorial: Build your first neural network from scratch'
date: 2023-08-17
permalink: /posts/2023/08/blog-post/
tags:
  - cool posts
  - category1
  - category2
---

This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. 

Headings are cool
======
In this episode, we want to investigate a simple working neural network (NN) anatomically, without the use of any deep learning package. It's very interesting to design every single element that is essential for performing forward and backward propagation. Every piece of component can be put up together like playing a LEGO game!  
we are going to implement a $L$ layer neural network (multiple layer perceptron) that can perform simple tasks such as digit recognization. $L$ is an arbitrary number as you require, within which we want $L-1$ weight matrices $\boldsymbol{w}^{(i)}$ that link the $i$-th and the $i+1$-th layer, and the last one is the output layer (e.g. a Softmax). We define a data matrix $\boldsymbol{X}$ ($N$ by $M$) that contains $N$ pieces of data with dimension $M$, and a target $\boldsymbol{Y}$ ($N$ by $K$), a matrix of $K$-category one-hot label. We follow \cite{xu} step by step via specifying every component and giving MATLAB implementation. Some definitions of variables.
$\boldsymbol{z}^{(i)}=\boldsymbol{a}^{(i-1)}\boldsymbol{w}^{(i-1)}$, pre-activation at the $i$-th layer;  
$\boldsymbol{a}^{(i)}=\sigma(\boldsymbol{z}^{(i)})$, activated product at the $i$-th layer;  
$\alpha$, constant learning rate;  
Since we are dealing with data matrices rather than vectors, to make more efficient computation we try to enable every MATLAB function to do element-wise computation with matrices.
$$\begin{equation}
\sigma(t)=\frac{1}{1+\exp (-t)}
\end{equation}$$
```
function g = sigmoid(T)
    g = 1 ./ (1+exp(-T));
end
```
$$\begin{align}\frac{d \sigma(t)}{d t}=\sigma(t)(1-\sigma(t))$\end{align}$



These render differently. For example, type the following to show inline mode: $\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$
$$\sum_{i=0}^n i^2 = \frac{(n^2+n)(2n+1)}{6}$$
$$\left.x^2\right\rvert_3^5 = 5^2-3^2$$  

$$\begin{align} V_{sphere} = \frac{4}{3}\pi r^3\end{align}$$
This is a sample blog post.
$$ \begin{pmatrix} 1 & x & x^2\\ 1 & y & y^2\\ 1 & z & z^2\\ \end{pmatrix} $$

This is a sample blog post.
```  
function [C,a,z] = forward(X,Y,layers,weights)
a1 = [ones(size(X,1), 1) X];                %design input
C = 0;                                      %initialise cost

for i = 1:(size(layers,2))                  %initialize the sum and the activations
    z{i} = zeros(size(X,1),layers(i)+1);
    a{i} = zeros(size(X,1),layers(i)+1);
end

a{1} = a1;                                  %input layer
for i = 2:size(layers,2)                    %hidden layers
z{i} = a{i-1}*weights{i-1}';
a{i} = z{i};
if i ~= size(layers,2)                       %(eq 1.6)
    a{i} = sigmoid(z{i});
    a{i} = [ones(length(z{i}),1) a{i}];     %append bias for next layer
    else
    a{i} = softmax(a{i});                   %output layer (eq 1.7)
    end
end

C = crossentropy(Y,a{end});                 %compute cost eq.(eq 1.8)
end  
```
This is a sample blog post.

$$\begin{pmatrix}a & b\\ c & d\end{pmatrix}$$
This is a sample blog post.
$$\begin{align}\begin{bmatrix}X\\Y\end{bmatrix}\end{align}$$
This is a sample blog post.
\begin{align}
\frac{\partial}{\partial\boldsymbol{w}} \mathcal{C}(\boldsymbol{w}) &=\frac{\partial}{\partial\boldsymbol{w}} 
\end{align}

You can have many headings
======
This is a sample blog post.
$$\begin{align}
  X_0 \otimes X_1 =  
  \begin{pmatrix}
    0\begin{pmatrix} 
      0 & 1 \\
      1 & 0
    \end{pmatrix} & 1\begin{pmatrix}
                      0 & 1 \\
                      1 & 0
                     \end{pmatrix} \\
    1\begin{pmatrix} 
      0 & 1 \\
      1 & 0
     \end{pmatrix} & 0\begin{pmatrix}
                       0 & 1 \\
                       1 & 0
                      \end{pmatrix} 
  \end{pmatrix} = 
  \begin{pmatrix}
    0 & 0 & 0 & 1 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    1 & 0 & 0 & 0 
  \end{pmatrix}
\end{align}$$


This is a sample blog post.
$$\begin{align}
\frac{\partial}{\partial\boldsymbol{w}} \mathcal{C}(\boldsymbol{w}) &=\frac{\partial}{\partial\boldsymbol{w}} \left(\frac{1}{N}\sum_{i=1}^N \sum_{k=1}^K -y_{i, k}\left[\log \left(\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_k\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)}\right)\right]\right) \\
&=\frac{1}{N}\left[\begin{array}{ccc}
\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_1\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)} &
\cdots &
\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_K\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)}
\end{array}\right]-\frac{1}{N}\boldsymbol{y}_i \\
& \equiv \frac{1}{N} \left[\frac{\exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}\right)}{\sum_{l=1}^K \exp \left(\mathbf{a}_i \boldsymbol{w}^{\top}_l\right)}-\boldsymbol{y}_i\right]\\
&  \equiv \frac{1}{N}\left(\boldsymbol{a}^{(L)}-\boldsymbol{Y}\right).
\end{align}$$
This is a sample blog post.


Aren't headings cool?
------
